
keyword
keyword 是一种数据类型，用于存储结构化的文本数据（如标签、URL、邮箱、状态码等），其特点是不进行分词，直接将整个文本作为一个完整的词条进行索引。这使得 keyword 类型非常适合用于过滤、排序、聚合等操作。
核心特点
1.不分词（Exact Value）
输入的文本会被完整保留，不会被拆分为多个词条。例如，"New York" 会被索引为一个整体，而不是 "New" 和 "York"。
2.支持精确匹配
适合用于精确查找（如 term 查询）、分组统计（聚合）和排序。
3.默认限制长度
单个 keyword 字段的最大长度默认为 32766 字节（32mb），超过会导致索引失败（可通过 ignore_above 参数调整）。

与 text 类型的关键区别
特性	                    keyword	                        text
分词处理	                    不分词，保留完整文本	            分词（根据分词器拆分为多个词条）
适合操作	                    精确匹配、过滤、排序、聚合	        全文搜索（模糊匹配）
索引大小	                    通常较小（词条少）	            通常较大（词条多）
默认搜索方式	                精确匹配（term 查询）	            模糊匹配（match 查询）

若需同时支持全文搜索和精确匹配，可在 text 字段下添加 keyword 子字段（如 name.keyword）。
id类型的字段几乎全部都使用keyword

scroll
适合海量数据导出，不适合实时查询。拍快照，不能反映最新数据。适用于离线分析，全量数据同步

分页
若需实时分页，且结果集较小（< 10,000），可使用 from/size（但深度分页性能差）。官方建议避免超过 10,000 条记录的深度分页
理论上支持任意的偏移量，但实际系统有限制（默认from超过10000时会报错，可以调整系统参数来解除限制，但会显著增加集群内存和 CPU 消耗，不建议在生产环境中操作。）
按价格降序排列，获取第 51-60 条记录：
GET /products/_search
{
  "from": 50,
  "size": 10,
  "sort": [
    {
      "price": {
        "order": "desc"
      }
    }
  ],
  "query": {
    "term": {
      "category": "electronics"
    }
  }
}

深分页
ES 5.0+ 推荐使用 search_after 参数。search_after 是一种高性能的分页方式，用于替代传统的 from/size 分页。它通过维护上一页的排序值（而非页码）来实现高效的深度分页，特别适合处理海量数据和实时更新的场景。
不支持任意的偏移量
核心原理
1.排序依赖：必须指定至少一个排序字段（如时间戳、ID），结果按此排序。
2.状态传递：每次查询返回最后一条记录的排序值，客户端将其作为 search_after 参数传递给下一次请求。
3.无深度分页问题：无需维护全局页码，每次查询仅关注当前页之后的数据，避免 from/size 的性能开销。
注意：
1.排序字段必须包含唯一性：若排序字段值可能重复（如时间戳），需添加辅助字段（如 _id）确保唯一性，否则可能导致数据丢失。
2.不支持随机访问：search_after 只能按顺序向后翻页，无法直接跳转到指定页（需从头开始遍历）。
3.性能优化：优先使用 keyword 或数字类型字段排序（避免 text 字段的分词开销）。避免在高基数字段（如用户 ID）上进行深度分页。
4.实时性保证：每次查询反映最新数据状态（与 scroll 的快照机制不同）。

示例
1. 首次查询（获取第一页）
GET /products/_search
{
  "size": 10,  # 每页10条记录
  "sort": [
    {"timestamp": "desc"},  # 主排序字段（时间戳降序）
    {"_id": "asc"}          # 辅助排序字段（确保唯一性）
  ],
  "query": {
    "match": {
      "category": "electronics"
    }
  }
}
返回
{
  "hits": {
    "hits": [
      {
        "_id": "1001",
        "timestamp": 1680000000,
        "sort": [1680000000, "1001"]  // 最后一条记录的排序值
      },
      // ... 其他9条记录 ...
      {
        "_id": "1010",
        "timestamp": 1679999990,
        "sort": [1679999990, "1010"]  // 最后一条记录的排序值，用于下次查询
      }
    ]
  }
}
2. 使用 search_after 获取下一页
GET /products/_search
{
  "size": 10,
  "sort": [
    {"timestamp": "desc"},
    {"_id": "asc"}
  ],
  "search_after": [1679999990, "1010"],  // 上一页最后一条记录的排序值
  "query": {
    "match": {
      "category": "electronics"
    }
  }
}

es评分
评分计算是指在执行查询时，如何评估文档与查询的相关性，并为每个匹配的文档分配一个分数（_score）。这个分数越高，表示文档与查询的相关性越强。ES 默认使用 BM25 算法（从 Elasticsearch 5.0 版本开始）来计算评分
评分的基本原理
词频（Term Frequency, TF）：词项在文档中出现的次数。出现次数越多，相关性越高。
逆文档频率（Inverse Document Frequency, IDF）：词项在整个索引中的稀有性。越稀有（即越少文档包含该词），相关性越高。
字段长度归一化（Field-length Normalization）：字段越短，词项的重要性越高。例如，一个词在短标题中出现比在长正文中出现更有价值。
查询条件的组合：复合查询（如bool查询）会综合多个子查询的评分，使用不同的组合方式（如must、should、filter）。

可以使用explain参数来查看评分的详细计算过程：
GET /my_index/_search?explain
{
  "query": {
    "match": {
      "title": "Elasticsearch"
    }
  }
}

term查询
es通过term查询时，直接查询倒排索引中的词项。一般用于精准匹配
查询的字段最好设置为keyword（如果是text，查询结果可能不符合预期）
需确保查询词项与索引中的分词结果完全一致，否则无法命中
默认分词器会把中文分成单个汉字，因此查询类似“中国”会查询不到，但查询“中”能查到
默认分词器会把大写转成小写，去掉“the”之类的单词。假设有文档{"content": "Hello World"}
则查询条件为"term": { "content": "Hello" }时无法查询到，必须使用hello才能

match查询
先对输入文本进行分词，再查每个词项（也是通过倒排索引），再汇总，默认按照score排序。一般用于全文搜索
比如文档包含“中文”，“中文2”，“中2”。查询“中文”可以将3条全部查询出来

综上，term最好在keyword字段上查询，而match最好在text字段上查询

IK分词器
精准分词（ik_smart）：按最可能的方式切分文本。
示例："我爱北京天安门" → ["我", "爱", "北京", "天安门"]

细粒度分词（ik_max_word）：尽可能多的切分词语。
示例："我爱北京天安门" → ["我", "爱", "北京", "天安门", "天安", "门"]

